{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f9e283-9d66-4026-ba20-251c3c64c583",
   "metadata": {},
   "source": [
    "# Imports and opening the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b61bdc35-2d67-410b-a1cb-bce43f597e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import datapurifier as dp\n",
    "from datapurifier import Mleda, Nlpurifier, NLAutoPurifier, MlReport, Nlpeda\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "StopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "898b96a7-5418-41c9-b998-fbebd4da0811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2811773, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@sprintcare and how do you propose we do that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@sprintcare I have sent several private messag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@115712 Please send us a Private Message so th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sprintcare I did.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@115712 Can you please send us a private messa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@sprintcare is the worst customer service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@115713 This is saddening to hear. Please shoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@sprintcare You gonna magically change your co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@115713 We understand your concerns and we'd l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@sprintcare Since I signed up with you....Sinc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0      @sprintcare and how do you propose we do that\n",
       "1  @sprintcare I have sent several private messag...\n",
       "2  @115712 Please send us a Private Message so th...\n",
       "3                                 @sprintcare I did.\n",
       "4  @115712 Can you please send us a private messa...\n",
       "5          @sprintcare is the worst customer service\n",
       "6  @115713 This is saddening to hear. Please shoo...\n",
       "7  @sprintcare You gonna magically change your co...\n",
       "8  @115713 We understand your concerns and we'd l...\n",
       "9  @sprintcare Since I signed up with you....Sinc..."
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open file in df\n",
    "\n",
    "path = \"data/data.csv\"\n",
    "full_data_df = pd.read_csv(path)\n",
    "full_data_df.columns=[\"text\"]\n",
    "print(full_data_df.shape)\n",
    "\n",
    "# create small df for testing\n",
    "data_df = full_data_df.head(40000)\n",
    "data_df.columns=[\"text\"]\n",
    "\n",
    "# print first 10 lines for inspection\n",
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f55ff3d1-ae5b-4f74-bf39-0237925759ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split costomer questions from employee replies\n",
    "\n",
    "instance_list = data_df['text'].tolist()\n",
    "\n",
    "customer_instances = []\n",
    "\n",
    "for item in instance_list:\n",
    "    if item[1].isalpha() == True:\n",
    "        customer_instances.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e55074c7-9130-4d65-bfdf-a3d55ffacede",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_customer_instances = ' '.join(customer_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0365caf-6033-4a3a-ab8d-f0dc5d94e76e",
   "metadata": {},
   "source": [
    "# Pre-preprocessing\n",
    "\n",
    "The dataset is too large for my computer to handle, but I did not want to lose valuable information by simply cutting it in half. So before I started preprocessing with Spacy and other heavy modules, I manually split the data into words and found the most frequent keywords. I extracted all instances that contain the found keywords to shorten the data. This step is also helpful to already create a dataset focused on a few common topics, which will make it easier for the clustering algorithm to find clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b39528fa-e3cc-4d46-a03a-e6343b5c1c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shorten and precluster data by frequent keywords\n",
    "\n",
    "# # open file\n",
    "# path = \"data/data.csv\"\n",
    "# with open(path, \"r\", encoding='utf-8') as infile:\n",
    "#     raw_content = infile.read()\n",
    "    \n",
    "# preclean and split content\n",
    "new_content = joined_customer_instances.replace('\\n', '. ')\n",
    "# new_content = raw_content.replace('\\n', '. ')\n",
    "new_content = new_content.replace('..', '.')\n",
    "new_content = new_content.replace('  ', ' ')\n",
    "new_content = new_content.replace('\"', '')\n",
    "split_content = new_content.split()\n",
    "\n",
    "# get most frequent keywords\n",
    "freq_counts = Counter(list(split_content))\n",
    "\n",
    "# remove stopwords from frequency dict\n",
    "for word in StopWords:\n",
    "    if word in freq_counts:\n",
    "        del freq_counts[word]\n",
    "        \n",
    "most_common = freq_counts.most_common(200)\n",
    "\n",
    "# get 100 most common words in list for viewing\n",
    "most_common_words = []\n",
    "for item in most_common:\n",
    "    most_common_words.append(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "29df3ccd-d79b-4511-8470-6d5ec08a76dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', '@AmazonHelp', '@AppleSupport', 'get', '.', 'still', '@ChipotleTweets', 'service', '@British_Airways', '-', '&amp;', 'time', 'one', 'Thanks', 'got', '@Uber_Support', 'back', 'The', '@sainsburys', 'need', \"I'm\", '@Tesco', 'Thank', 'phone', 'like', '2', 'help', 'flight', 'know', '@AmericanAir', 'customer', 'It', 'would', '@O2', 'I’m', 'please', 'My', 'No', 'want', 'going', 'new', '@Delta', '@VirginTrains', 'account', 'email', 'call', 'getting', 'This', 'even', '@AskPlayStation', 'it’s', 'app', '@SouthwestAir', '@SpotifyCares', 'You', \"can't\", '@GWRHelp', '@Ask_Spectrum', 'How', '@AirAsiaSupport', 'u', 'order', '@idea_cares', 'Is', 'What', 'Can', 'day', \"I've\", 'Why', 'Just', 'see', 'So', '@VerizonSupport', 'last', 'sent', '@Safaricom_Care', '@hulu_support', 'go', 'already', 'check', '@XboxSupport', 'Hey', 'issue', 'update', 'Not', 'since', 'Please', 'iPhone', 'use', 'make', 'thanks', 'Hi', 'guys', 'And', '@SW_Help', 'trying', 'could', 'it.', 'says', 'days', 'number', 'said', '3', 'tried', '?', 'store', 'I’ve', '@comcastcares', 'don’t', 'really', 'work', 'people', 'pay', \"It's\", 'When', 'delivery', '@sprintcare', 'way', 'today', 'internet', 'But', 'never', 'can’t', 'told', 'card', 'working', '@MicrosoftHelps', 'able', '@marksandspencer', '@Morrisons', 'problem', 'fix', 'change', 'received', 'If', 'money', 'take', 'much', 'give', 'train', '@TMobileHelp', 'DM', 'find', 'someone', 'us', 'It’s', 'also', 'refund', 'using', 'every', 'iOS', 'Amazon', 'send', 'good', 'contact', 'let', 'keep', '4', 'thank', 'times', 'long', 'next', 'think', 'waiting', 'tell', '@AskeBay', '1', 'Yes', 'another', 'de', 'say', '@115858', 'done', 'you.', 'two', 'home', '@AldiUK', 'driver', 'support', 'first', 'come', 'message', 'me.', '@CoxHelp', '@ArgosHelpers', 'buy', 'sure', 'great', 'We', 'Your', 'wait', ',', 'nothing', 'error', 'try', 'via', 'They', 'Any', 'I️', 'address']\n"
     ]
    }
   ],
   "source": [
    "# view most common words\n",
    "print(most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d5f624f3-b215-4caa-81a6-d202b0cb55da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define keywords to filter data before heavy preprocessing\n",
    "\n",
    "keywords = ['phone', 'flight', 'account', 'order', 'issue', 'iphone', 'delivery', 'internet', 'card', 'working', 'train', 'refund']\n",
    "\n",
    "# keywords = ['phone', 'email', 'account', 'number', 'team', 'order', 'issue', 'address', 'details',\n",
    "#            'contact', 'flight', 'link', 'booking', 'baggage', 'call', 'cancel', 'delivery', 'app', 'update', 'iphone', 'refund', 'due']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fc9d7911-dc71-4d17-93c9-d47a211983c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "388596"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get instances into list\n",
    "instance_list = data_df['text'].tolist()\n",
    "\n",
    "# split costomer questions from employee replies\n",
    "customer_instances = []\n",
    "\n",
    "for item in instance_list:\n",
    "    if item[1].isalpha() == True:\n",
    "        customer_instances.append(item)\n",
    "\n",
    "# filter instances by keyword\n",
    "filtered_instances = []\n",
    "\n",
    "for instance in customer_instances:\n",
    "    split_instance = instance.split()\n",
    "    for word in split_instance:\n",
    "        if word.lower() in keywords:\n",
    "            filtered_instances.append(instance)\n",
    "            break\n",
    "\n",
    "# join for further preprocessing            \n",
    "joined_filtered_data = '. '.join(filtered_instances)\n",
    "len(joined_filtered_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c544cbcc-fc15-430e-85fa-d8be762a6e6b",
   "metadata": {},
   "source": [
    "# Preprocessing and cleaning\n",
    "\n",
    "Now that we have a filtered, shortened dataset, we can process it with SpaCy and DataPurifier. After cleaning, the data is saved to use in the clustering pipelines. The following cleaning steps are performed:\n",
    "\n",
    "- Lemmatize\n",
    "- Remove stop words\n",
    "- Remove special characters, numeric characters and punctuation\n",
    "- Remove links and tokens starting with '@'\n",
    "- Remove tokens starting with a '-' or '^' as those are employee names\n",
    "- Strip leading, trailing and duplicate whitespaces\n",
    "- Remove most frequent and least frequent words with a threshold of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "06198598-3fba-4105-ab13-018516176235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1595, 1)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load filtered data in spacy\n",
    "\n",
    "# shorten data to spacy max length\n",
    "joined_filtered_data = joined_filtered_data[:100000]\n",
    "doc = nlp(joined_filtered_data)\n",
    "\n",
    "# get all spacy sentences in list\n",
    "sents = []\n",
    "\n",
    "for sent in doc.sents:\n",
    "    sent_list = []\n",
    "    for token in sent:\n",
    "        token = str(token)\n",
    "        # removing tokens that datapurifier will not remove\n",
    "        if token.startswith('-') == False and token.startswith('^') == False and token.startswith('@') == False and token != \" \" and token != \"\" and token != '\\n':\n",
    "            sent_list.append(token)\n",
    "    sent_list = ' '.join(sent_list)\n",
    "    sents.append(sent_list)\n",
    "    \n",
    "# remove super short sentences\n",
    "sentences = []\n",
    "for item in sents:\n",
    "    if len(item) > 3:\n",
    "        sentences.append(item)\n",
    "        \n",
    "# get sentences in dataframe for further processing\n",
    "d = {'sentences': sentences}\n",
    "filtered_df = pd.DataFrame(d)\n",
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "af86ed02-6c9e-4d8d-a04a-ab84f5a46cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8687026e094a359734a8041de9bd5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridspecLayout(children=(Checkbox(value=False, description='Drop Null Rows', indent=False, layout=Layout(grid_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34m\n",
      "Convert Word to its Base Form\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886cc2f16cd64505a548742c8e63050d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(RadioButtons(description='Technique:', options=('None', 'Stemming', 'Lemmatization'), va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mRemove Top Common Words\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb6837934b747218fb5b3d5b5043021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='Remove Top Common Words'), Output()), _dom_classes=('…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mRemove Top Rare Words\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad64987547884159a169b6257f49a307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='Remove Top Rare Words'), Output()), _dom_classes=('wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0770124073284928abfd46d75f5633dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Start Purifying', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# clean data with DataPurifier\n",
    "cleaned_df = Nlpurifier(filtered_df, \"sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "daa977be-a884-49b1-b430-d17dcf32b58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>raw_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>correct way ocs account takeover email consent...</td>\n",
       "      <td>The correct way to do it is via an OCS Account...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>friend internet need play videogame skill dimi...</td>\n",
       "      <td>My friend is without internet we need to play ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>How ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phone number email</td>\n",
       "      <td>I have my phone number and email , that 's it .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>equipment service</td>\n",
       "      <td>How did I get equipment and service ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>literally try pay find</td>\n",
       "      <td>I 'm literally trying to pay and nobody can fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>thank resolve issue quickly</td>\n",
       "      <td>Thank you for resolving my issue so quickly ! !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>y all good   fanforlife</td>\n",
       "      <td>Y’all are the best ☺ ️ # fanforlife .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>frustrated order dinner saturday app</td>\n",
       "      <td>So frustrated with 😡 Ordered dinner on Saturda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>order wrong charge credit card twice</td>\n",
       "      <td>Order was wrong AND they charged my credit car...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences  \\\n",
       "0  correct way ocs account takeover email consent...   \n",
       "1  friend internet need play videogame skill dimi...   \n",
       "2                                                      \n",
       "3                              phone number email      \n",
       "4                                 equipment service    \n",
       "5                          literally try pay find      \n",
       "6                     thank resolve issue quickly      \n",
       "7                           y all good   fanforlife    \n",
       "8              frustrated order dinner saturday app    \n",
       "9              order wrong charge credit card twice    \n",
       "\n",
       "                                       raw_sentences  \n",
       "0  The correct way to do it is via an OCS Account...  \n",
       "1  My friend is without internet we need to play ...  \n",
       "2                                              How ?  \n",
       "3    I have my phone number and email , that 's it .  \n",
       "4              How did I get equipment and service ?  \n",
       "5  I 'm literally trying to pay and nobody can fi...  \n",
       "6    Thank you for resolving my issue so quickly ! !  \n",
       "7              Y’all are the best ☺ ️ # fanforlife .  \n",
       "8  So frustrated with 😡 Ordered dinner on Saturda...  \n",
       "9  Order was wrong AND they charged my credit car...  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview cleaned df\n",
    "cleaned_df = cleaned_df.df\n",
    "cleaned_df['raw_sentences'] = sentences # add original sentences\n",
    "\n",
    "cleaned_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "98880a3a-3ae0-4843-983a-59ed8466f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered and cleaned df for use in other notebooks\n",
    "cleaned_df.to_csv(\"data/cleaned_customer_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38e1a6c-23de-4406-9c51-b0a054f76feb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
