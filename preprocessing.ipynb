{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f9e283-9d66-4026-ba20-251c3c64c583",
   "metadata": {},
   "source": [
    "# Imports and opening the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b61bdc35-2d67-410b-a1cb-bce43f597e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shark\\anaconda3\\lib\\site-packages\\spacy\\util.py:758: UserWarning: [W095] Model 'en_core_web_sm' (3.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.12.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.12.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import datapurifier as dp\n",
    "from datapurifier import Mleda, Nlpurifier, NLAutoPurifier, MlReport, Nlpeda\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "StopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "898b96a7-5418-41c9-b998-fbebd4da0811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2811773, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@sprintcare and how do you propose we do that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@sprintcare I have sent several private messag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@115712 Please send us a Private Message so th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sprintcare I did.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@115712 Can you please send us a private messa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@sprintcare is the worst customer service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@115713 This is saddening to hear. Please shoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@sprintcare You gonna magically change your co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@115713 We understand your concerns and we'd l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@sprintcare Since I signed up with you....Sinc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0      @sprintcare and how do you propose we do that\n",
       "1  @sprintcare I have sent several private messag...\n",
       "2  @115712 Please send us a Private Message so th...\n",
       "3                                 @sprintcare I did.\n",
       "4  @115712 Can you please send us a private messa...\n",
       "5          @sprintcare is the worst customer service\n",
       "6  @115713 This is saddening to hear. Please shoo...\n",
       "7  @sprintcare You gonna magically change your co...\n",
       "8  @115713 We understand your concerns and we'd l...\n",
       "9  @sprintcare Since I signed up with you....Sinc..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open file in df\n",
    "\n",
    "path = \"data/data.csv\"\n",
    "full_data_df = pd.read_csv(path)\n",
    "full_data_df.columns=[\"text\"]\n",
    "print(full_data_df.shape)\n",
    "\n",
    "# create small df for testing\n",
    "data_df = full_data_df.head(40000)\n",
    "data_df.columns=[\"text\"]\n",
    "\n",
    "# print first 10 lines for inspection\n",
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0365caf-6033-4a3a-ab8d-f0dc5d94e76e",
   "metadata": {},
   "source": [
    "# Pre-preprocessing\n",
    "\n",
    "The dataset is too large for my computer to handle, but I did not want to lose valuable information by simply cutting it in half. So before I started preprocessing with Spacy and other heavy modules, I manually split the data into words and found the most frequent keywords. I extracted all instances that contain the found keywords to shorten the data. This step is also helpful to already create a dataset focused on a few common topics, which will make it easier for the clustering algorithm to find clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b39528fa-e3cc-4d46-a03a-e6343b5c1c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shorten and precluster data by frequent keywords\n",
    "\n",
    "# open file\n",
    "path = \"data/data.csv\"\n",
    "with open(path, \"r\", encoding='utf-8') as infile:\n",
    "    raw_content = infile.read()\n",
    "    \n",
    "# preclean and split content\n",
    "new_content = raw_content.replace('\\n', '. ')\n",
    "new_content = new_content.replace('..', '.')\n",
    "new_content = new_content.replace('  ', ' ')\n",
    "new_content = new_content.replace('\"', '')\n",
    "split_content = new_content.split()\n",
    "\n",
    "# get most frequent keywords\n",
    "freq_counts = Counter(list(split_content))\n",
    "\n",
    "# remove stopwords from frequency dict\n",
    "for word in StopWords:\n",
    "    if word in freq_counts:\n",
    "        del freq_counts[word]\n",
    "        \n",
    "most_common = freq_counts.most_common(200)\n",
    "\n",
    "# get 100 most common words in list for viewing\n",
    "most_common_words = []\n",
    "for item in most_common:\n",
    "    most_common_words.append(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29df3ccd-d79b-4511-8470-6d5ec08a76dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'us', 'DM', 'Please', 'We', 'get', 'Hi', 'like', 'please', '@AmazonHelp', 'look', 'know', 'you.', 'sorry', 'help', \"I'm\", '-', 'Thanks', 'send', '.', 'Can', 'service', 'need', 'phone', 'would', 'If', '@AppleSupport', 'Thank', 'email', 'account', 'number', \"We're\", '&amp;', 'see', 'back', 'still', 'hear', 'You', 'want', 'take', 'team', 'here:', 'time', 'help.', 'check', 'let', 'The', 'order', 'issue', 'via', 'one', 'Hey', 'address', 'happy', 'details', 'Sorry', 'This', \"we'll\", 'new', 'able', 'What', 'got', 'make', 'going', 'https://t.co/GDrqU22YpT.', 'contact', 'this.', 'flight', 'customer', 'It', 'assist', \"We'll\", \"We'd\", '2', '@AmericanAir', 'Hi,', 'My', '@Uber_Support', 'sure', 'link', 'getting', 'go', 'work', 'I’m', 'use', 'call', 'full', 'it.', 'store', 'Have', 'No', 'delivery', 'could', 'Is', '@Delta', \"can't\", 'try', 'working', 'de', 'Send', 'love', 'even', 'app', 'provide', 'iPhone', 'update', 'reach', 'it’s', 'follow', '@VirginTrains', 'How', 'using', 'iOS', 'issues', \"I've\", 'support', 'may', 'way', 'So', '@115858', 'last', 'really', '@Tesco', 'there.', 'fix', 'find', 'Let', 'sent', 'great', '@SouthwestAir', 'trying', 'Just', 'help!', \"Let's\", 'day', 'keep', 'info', 'Our', 'never', '@SpotifyCares', '@British_Airways', 'right', 'available', 'tell', 'DM.', 'Could', 'free', 'since', 'give', 'us.', 'received', 'message', \"I'll\", 'thanks', 'there,', 'information', 'always', '@XboxSupport', 'share', 'tried', '3', 'there!', 'anything', 'reaching', 'Why', 'also', 'good', 'Are', '@GWRHelp', \"It's\", 'card', 'already', 'next', 'Thanks.', 'guys', 'When', 'out.', \"we're\", 'best', 'apologize', 'days', 'feedback', 'pay', 'Oh', '@ATVIAssist', 'don’t', 'Not', 'Do', 'much', \"That's\", 'says', 'internet', 'train', 'experience', '@115913', 'u', 'hope', 'appreciate', 'change', 'another']\n"
     ]
    }
   ],
   "source": [
    "# view most common words\n",
    "print(most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5f624f3-b215-4caa-81a6-d202b0cb55da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define keywords to filter data before heavy preprocessing\n",
    "\n",
    "keywords = ['phone', 'email', 'account', 'number', 'team', 'order', 'issue', 'address', 'details',\n",
    "           'contact', 'flight', 'link', 'booking', 'baggage', 'call', 'cancel', 'delivery', 'app', 'update', 'iphone', 'refund', 'due']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc9d7911-dc71-4d17-93c9-d47a211983c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514521"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get instances into list\n",
    "instance_list = data_df['text'].tolist()\n",
    "\n",
    "# split costomer questions from employee replies\n",
    "customer_instances = []\n",
    "\n",
    "for item in instance_list:\n",
    "    if item[1].isalpha() == True:\n",
    "        customer_instances.append(item)\n",
    "\n",
    "# filter instances by keyword\n",
    "filtered_instances = []\n",
    "\n",
    "for instance in customer_instances:\n",
    "    split_instance = instance.split()\n",
    "    for word in split_instance:\n",
    "        if word.lower() in keywords:\n",
    "            filtered_instances.append(instance)\n",
    "            break\n",
    "\n",
    "# join for further preprocessing            \n",
    "joined_filtered_data = '. '.join(filtered_instances)\n",
    "len(joined_filtered_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c544cbcc-fc15-430e-85fa-d8be762a6e6b",
   "metadata": {},
   "source": [
    "# Pre-processing and cleaning\n",
    "\n",
    "Now that we have a filtered, shortened, dataset, we can process it with SpaCy and DataPurifier. After cleaning, the data is saved to use in the clustering pipelines. The following cleaning steps are performed:\n",
    "\n",
    "- Lemmatize\n",
    "- Lowercase\n",
    "- Remove stop words\n",
    "- Remove special characters, numeric characters and punctuation\n",
    "- Remove links and tokens starting with '@'\n",
    "- Remove tokens starting with a '-' or '^' as those are employee names\n",
    "- Strip leading, trailing and duplicate whitespaces\n",
    "- Remove most frequent and least frequent words with a threshold of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06198598-3fba-4105-ab13-018516176235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1582, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load filtered data in spacy\n",
    "\n",
    "# shorten data to spacy max length\n",
    "joined_filtered_data = joined_filtered_data[:100000]\n",
    "doc = nlp(joined_filtered_data)\n",
    "\n",
    "# get all spacy sentences in list\n",
    "sents = []\n",
    "\n",
    "for sent in doc.sents:\n",
    "    sent_list = []\n",
    "    for token in sent:\n",
    "        token = str(token)\n",
    "        # removing tokens that datapurifier will not remove\n",
    "        if token.startswith('-') == False and token.startswith('^') == False and token.startswith('@') == False and token != \" \" and token != \"\" and token != '\\n':\n",
    "            sent_list.append(token)\n",
    "    sent_list = ' '.join(sent_list)\n",
    "    sents.append(sent_list)\n",
    "    \n",
    "# remove super short sentences\n",
    "sentences = []\n",
    "for item in sents:\n",
    "    if len(item) > 3:\n",
    "        sentences.append(item)\n",
    "        \n",
    "# get sentences in dataframe for further processing\n",
    "d = {'sentences': sentences}\n",
    "filtered_df = pd.DataFrame(d)\n",
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af86ed02-6c9e-4d8d-a04a-ab84f5a46cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed29657084640799e76db919e9f8f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridspecLayout(children=(Checkbox(value=False, description='Drop Null Rows', indent=False, layout=Layout(grid_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34m\n",
      "Convert Word to its Base Form\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a52cfde7f1e4dfb8311f637c90d2c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(RadioButtons(description='Technique:', options=('None', 'Stemming', 'Lemmatization'), va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mRemove Top Common Words\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc3ead31773430e91650b3b375ce033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='Remove Top Common Words'), Output()), _dom_classes=('…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mRemove Top Rare Words\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ee700998eb4b31b10d15e06b07b5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='Remove Top Rare Words'), Output()), _dom_classes=('wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c589f6ea34c5410cb954287e4cf10476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Start Purifying', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# clean data with DataPurifier\n",
    "cleaned_df = Nlpurifier(filtered_df, \"sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daa977be-a884-49b1-b430-d17dcf32b58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>raw_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>like email copy spectrum update training</td>\n",
       "      <td>Would you like me to email you a copy of one s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>correct way ocs account takeover email consent...</td>\n",
       "      <td>The correct way to do it is via an OCS Account...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>actually break link send incorrect information</td>\n",
       "      <td>actually that 's a broken link you sent me and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>How ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phone number email</td>\n",
       "      <td>I have my phone number and email , that 's it .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>equipment service</td>\n",
       "      <td>How did I get equipment and service ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>literally try pay find</td>\n",
       "      <td>I 'm literally trying to pay and nobody can fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>thank resolve issue quickly</td>\n",
       "      <td>Thank you for resolving my issue so quickly ! !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>y all good   fanforlife</td>\n",
       "      <td>Y’all are the best ☺ ️ # fanforlife .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>frustrated order dinner saturday app</td>\n",
       "      <td>So frustrated with 😡 Ordered dinner on Saturda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences  \\\n",
       "0        like email copy spectrum update training      \n",
       "1  correct way ocs account takeover email consent...   \n",
       "2    actually break link send incorrect information    \n",
       "3                                                      \n",
       "4                              phone number email      \n",
       "5                                 equipment service    \n",
       "6                          literally try pay find      \n",
       "7                     thank resolve issue quickly      \n",
       "8                           y all good   fanforlife    \n",
       "9              frustrated order dinner saturday app    \n",
       "\n",
       "                                       raw_sentences  \n",
       "0  Would you like me to email you a copy of one s...  \n",
       "1  The correct way to do it is via an OCS Account...  \n",
       "2  actually that 's a broken link you sent me and...  \n",
       "3                                              How ?  \n",
       "4    I have my phone number and email , that 's it .  \n",
       "5              How did I get equipment and service ?  \n",
       "6  I 'm literally trying to pay and nobody can fi...  \n",
       "7    Thank you for resolving my issue so quickly ! !  \n",
       "8              Y’all are the best ☺ ️ # fanforlife .  \n",
       "9  So frustrated with 😡 Ordered dinner on Saturda...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview cleaned df\n",
    "clean_df = cleaned_df.df\n",
    "clean_df['raw_sentences'] = sentences # add original sentences\n",
    "\n",
    "clean_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98880a3a-3ae0-4843-983a-59ed8466f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered and cleaned df for use in other notebooks\n",
    "# clean_df.to_csv(\"data/cleaned_customer_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38e1a6c-23de-4406-9c51-b0a054f76feb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
